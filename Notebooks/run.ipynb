{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import transformers\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "CACHE_DIR = '/home/eklavya/Code/.cache'\n",
    "TOKEN = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model_id):\n",
    "    \n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        token=TOKEN,\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        token=TOKEN,\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "\n",
    "    generate_text = transformers.pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        trust_remote_code=True,\n",
    "        device_map={\"\": 0},\n",
    "        max_new_tokens = 300,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=generate_text, \n",
    "                              model_kwargs={'temperature':0.00})\n",
    "    \n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompt():\n",
    "    \n",
    "    examples =  [\n",
    "        {\n",
    "            'input': 'The subway is delayed yet again. This city just can not run on time. Apparently there is a water leak near Times Square. Feel so angry.',\n",
    "            'output':\n",
    "            \"\"\"\n",
    "            1. Subway\n",
    "            2. Negative\n",
    "            3. The subway was delayed due to a water leak near Times Square.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'input': \"It's such a beautiful day. Looks like I am going to go on a walk in Central Park.\",\n",
    "            'output':\n",
    "            \"\"\"\n",
    "            1. Unknown\n",
    "            2. Neutral\n",
    "            3. Doesn't talk about travel modes\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            'input': \"MTA has done a good job maintaining the bus service schedule this summer. I have been at work on time everyday.\",\n",
    "            'output':\n",
    "            \"\"\"\n",
    "            1. Bus\n",
    "            2. Positive\n",
    "            3. The bus service was well maintained throughout the summer.\n",
    "            \"\"\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt_template_cot = \"\"\"[INST]\n",
    "    Tweet: {input}\n",
    "\n",
    "    Question:\n",
    "    Only answer the following questions in order as bullet points.\n",
    "        1. Select the mode of travel: Subway, Bus, Bike, Taxi, Car, Unknown\n",
    "        2. Select the sentiment: Positive, Neutral, Negative\n",
    "        3. Explain your reasoning behind the selected sentiment in less than 20 words.\n",
    "\n",
    "    [/INST]auto\n",
    "    Answer:\n",
    "    {output}\n",
    "    \"\"\"\n",
    "\n",
    "    example_prompt = PromptTemplate(template=prompt_template_cot, input_variables=['input', 'output'])\n",
    "    \n",
    "    prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"\"\"\n",
    "    Tweet: {input}\n",
    "\n",
    "    Question:\n",
    "    Only answer the following questions in order as bullet points.\n",
    "        1. Select the mode of travel: Subway, Bus, Bike, Taxi, Car, Unknown\n",
    "        2. Select the sentiment: Positive, Neutral, Negative\n",
    "        3. Explain your reasoning behind the selected sentiment in less than 20 words.\n",
    "\n",
    "    Answer:\n",
    "    \"\"\",\n",
    "        input_variables=['input']\n",
    "    )\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_chain_fn(llm, prompt):\n",
    "    return LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "def do_qa(answer_list):\n",
    "\n",
    "    if len(answer_list) < 3:\n",
    "        return [None, None, None]\n",
    "\n",
    "    else:\n",
    "        if answer_list[0].strip().lower() not in ['subway', 'bus', 'bike', 'taxi', 'car', 'unknown']:\n",
    "            return ['Unknown', None, None]\n",
    "\n",
    "        if answer_list[1].strip().lower() not in ['positive', 'negative', 'neutral']:\n",
    "            return [answer_list[0], None, \"Can't determine sentiment\"]\n",
    "\n",
    "    return answer_list\n",
    "\n",
    "\n",
    "def process_answer(answer):\n",
    "    possible_answers = [' '.join(x.strip().split(\" \")[1:]) for x in answer.split(\"\\n\") if x.strip().startswith(tuple([\"1. \", \"2. \", \"3. \"]))]\n",
    "    final_answer = possible_answers[:3]\n",
    "\n",
    "    final_answer = do_qa(final_answer)\n",
    "    return final_answer\n",
    "\n",
    "def process_batch(results):\n",
    "    processed_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        processed_results.append([result['idx'], result['input'], process_answer(result['text'])])\n",
    "\n",
    "    return processed_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_results(llm_chain, src_path, dst_path, batch_size = 50):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(dst_path)\n",
    "        print(\"-- Output exists -- Beginning Batch Processing\")\n",
    "        # i = df_na.dropna(subset=['travel_mode']).shape[0]\n",
    "        # flag = True\n",
    "    except:\n",
    "        df = pd.read_csv(src_path)\n",
    "\n",
    "        print(\"-- Read data -- Begninning Batch Processing\")\n",
    "        df['travel_mode'] = None\n",
    "        df['sentiment'] = None\n",
    "        df['reasoning'] = None\n",
    "    \n",
    "    df_na = df[df['travel_mode'].isna()]\n",
    "    \n",
    "    print(f\"-- {len(df_na)} Number of Rows to be processed\")\n",
    "    inputs = []\n",
    "    \n",
    "    if df_na.empty:\n",
    "        return\n",
    "    \n",
    "    for i, x in zip(df.index, df['processed_txt'].values):\n",
    "        inputs.append({'input': x, 'idx': i})\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    \n",
    "    times = []\n",
    "    results = []\n",
    "    \n",
    "    i = df_na.index[0]\n",
    "        \n",
    "    while i < len(df_na.index):\n",
    "\n",
    "        print(f\" -- Starting Batch {i//batch_size + 1}\")\n",
    "        up = min(i + batch_size, len(df_na.index))\n",
    "        \n",
    "        batch_idx = df_na.index[i:up]\n",
    "        batch = list(inputs[batch_idx])\n",
    "        \n",
    "        start = time.time()\n",
    "        temp = llm_chain.batch(batch)\n",
    "        times.append(time.time() - start)\n",
    "\n",
    "        i = i + batch_size\n",
    "        print(f\" -- Completed Batch {i//batch_size}: {times[-1]}\")\n",
    "        \n",
    "        for res in temp:\n",
    "            df.iloc[res[0], -3:] = res[-1]\n",
    "        df.to_csv(dst_path, index=False)\n",
    "        \n",
    "        # result = process_batch(temp)\n",
    "        # results.extend(result)\n",
    "        \n",
    "        \n",
    "    # for res in results:\n",
    "    #     df.iloc[res[0], -3:] = res[-1]\n",
    "    df.to_csv(dst_path, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "# ----------------------------- MISTRAL 7B ---------------------\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name = 'mistral_7b_instruct'\n",
    "\n",
    "if not os.path.exists(\"/home/eklavya/Code/Results/\" + model_name):\n",
    "    os.mkdir(os.path.join(\"/home/eklavya/Code/Results/\", model_name))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------- LLAMA2 7B ---------------------\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = 'llama2_7b_chat'\n",
    "\n",
    "# if not os.path.exists(os.path.join(\"../Results/\", model_name)):\n",
    "#     os.mkdir(os.path.join(\"../Results/\", model_name))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm = get_model(model_id=model_id)\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.01)\n",
    "prompt = get_prompt()\n",
    "llm_chain = llm_chain_fn(llm=llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Output exists -- Beginning Batch Processing\n",
      "-- 0 Number of Rows to be processed\n"
     ]
    }
   ],
   "source": [
    "src_path1 = '/home/eklavya/Code/Data/processed_2000_2999.csv'\n",
    "dst_path1 = f'/home/eklavya/Code/Results/{model_name}/results_2000_2999.csv'\n",
    "\n",
    "get_results(llm_chain=llm_chain, src_path=src_path1, dst_path=dst_path1, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_path = '/home/eklavya/Code/Data/processed_3000_3999.csv'\n",
    "dst_path = f'/home/eklavya/Code/Results/{model_name}/results_3000_3999.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Read data -- Begninning Batch Processing\n",
      "-- 1000 Number of Rows to be processed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv(dst_path)\n",
    "    print(\"-- Output exists -- Beginning Batch Processing\")\n",
    "except:\n",
    "    df = pd.read_csv(src_path)\n",
    "\n",
    "    print(\"-- Read data -- Begninning Batch Processing\")\n",
    "    df['travel_mode'] = None\n",
    "    df['sentiment'] = None\n",
    "    df['reasoning'] = None\n",
    "\n",
    "df_na = df[df['travel_mode'].isna()]\n",
    "\n",
    "print(f\"-- {len(df_na)} Number of Rows to be processed\")\n",
    "inputs = []\n",
    "\n",
    "for i, x in zip(df.index, df['processed_txt'].values):\n",
    "    inputs.append({'input': x, 'idx': i})\n",
    "\n",
    "inputs = np.array(inputs)\n",
    "\n",
    "times = []\n",
    "results = []\n",
    "\n",
    "i = df_na.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Starting Batch 1\n",
      " -- Completed Batch 1: 1.4530143737792969\n",
      " -- Starting Batch 2\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/eklavya/Code/Notebooks/run.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binstance4-travel-modes/home/eklavya/Code/Notebooks/run.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(inputs[batch_idx])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binstance4-travel-modes/home/eklavya/Code/Notebooks/run.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Binstance4-travel-modes/home/eklavya/Code/Notebooks/run.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m temp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m llm_chain\u001b[39m.\u001b[39mabatch(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Binstance4-travel-modes/home/eklavya/Code/Notebooks/run.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m times\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Binstance4-travel-modes/home/eklavya/Code/Notebooks/run.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m result \u001b[39m=\u001b[39m process_batch(temp)\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/schema/runnable/base.py:477\u001b[0m, in \u001b[0;36mRunnable.abatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mainvoke(\u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    476\u001b[0m coros \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(ainvoke, inputs, configs)\n\u001b[0;32m--> 477\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m gather_with_concurrency(configs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_concurrency\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m*\u001b[39mcoros)\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/schema/runnable/utils.py:48\u001b[0m, in \u001b[0;36mgather_with_concurrency\u001b[0;34m(n, *coros)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gather coroutines with a limit on the number of concurrent coroutines.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mcoros)\n\u001b[1;32m     50\u001b[0m semaphore \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mSemaphore(n)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39m(gated_coro(semaphore, c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m coros))\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/schema/runnable/base.py:474\u001b[0m, in \u001b[0;36mRunnable.abatch.<locals>.ainvoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[39mreturn\u001b[39;00m e\n\u001b[1;32m    473\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mainvoke(\u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/chains/base.py:103\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mainvoke\u001b[39m(\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     98\u001b[0m     \u001b[39minput\u001b[39m: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     99\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    100\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    101\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    102\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macall(\n\u001b[1;32m    104\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m    105\u001b[0m         callbacks\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    106\u001b[0m         tags\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtags\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    107\u001b[0m         metadata\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    108\u001b[0m         run_name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    109\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    110\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/chains/base.py:379\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    378\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 379\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    380\u001b[0m \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    381\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    382\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/chains/base.py:373\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    366\u001b[0m run_manager \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    367\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    368\u001b[0m     inputs,\n\u001b[1;32m    369\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    371\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 373\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acall(inputs, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    374\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    375\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acall(inputs)\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    378\u001b[0m     \u001b[39mawait\u001b[39;00m run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:280\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_acall\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    278\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 280\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:147\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    145\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39magenerate_prompt(\n\u001b[1;32m    148\u001b[0m         prompts,\n\u001b[1;32m    149\u001b[0m         stop,\n\u001b[1;32m    150\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    151\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mabatch(\n\u001b[1;32m    155\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    156\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/base.py:517\u001b[0m, in \u001b[0;36mBaseLLM.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39magenerate_prompt\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    515\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    516\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magenerate(\n\u001b[1;32m    518\u001b[0m         prompt_strings, stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    519\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/base.py:823\u001b[0m, in \u001b[0;36mBaseLLM.agenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     run_managers \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    809\u001b[0m         \u001b[39m*\u001b[39m[\n\u001b[1;32m    810\u001b[0m             callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         ]\n\u001b[1;32m    821\u001b[0m     )\n\u001b[1;32m    822\u001b[0m     run_managers \u001b[39m=\u001b[39m [r[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m run_managers]\n\u001b[0;32m--> 823\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate_helper(\n\u001b[1;32m    824\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )\n\u001b[1;32m    826\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    827\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/base.py:711\u001b[0m, in \u001b[0;36mBaseLLM._agenerate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    708\u001b[0m     \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    709\u001b[0m         \u001b[39m*\u001b[39m[run_manager\u001b[39m.\u001b[39mon_llm_error(e) \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers]\n\u001b[1;32m    710\u001b[0m     )\n\u001b[0;32m--> 711\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    712\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    713\u001b[0m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    714\u001b[0m     \u001b[39m*\u001b[39m[\n\u001b[1;32m    715\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_end(flattened_output)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m     ]\n\u001b[1;32m    720\u001b[0m )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/base.py:698\u001b[0m, in \u001b[0;36mBaseLLM._agenerate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_agenerate_helper\u001b[39m(\n\u001b[1;32m    689\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    690\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    695\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    696\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 698\u001b[0m             \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(\n\u001b[1;32m    699\u001b[0m                 prompts,\n\u001b[1;32m    700\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[1;32m    701\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    702\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    703\u001b[0m             )\n\u001b[1;32m    704\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    705\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    708\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    709\u001b[0m             \u001b[39m*\u001b[39m[run_manager\u001b[39m.\u001b[39mon_llm_error(e) \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers]\n\u001b[1;32m    710\u001b[0m         )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:516\u001b[0m, in \u001b[0;36mBaseOpenAI._agenerate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    505\u001b[0m         {\n\u001b[1;32m    506\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m         }\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    517\u001b[0m         \u001b[39mself\u001b[39m, prompt\u001b[39m=\u001b[39m_prompts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    520\u001b[0m         response \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:132\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the async completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m llm\u001b[39m.\u001b[39masync_client\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    134\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    136\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    138\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/resources/completions.py:1127\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1086\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m   1087\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1126\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Completion \u001b[39m|\u001b[39m AsyncStream[Completion]:\n\u001b[0;32m-> 1127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m   1128\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/completions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1129\u001b[0m         body\u001b[39m=\u001b[39mmaybe_transform(\n\u001b[1;32m   1130\u001b[0m             {\n\u001b[1;32m   1131\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m   1132\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt,\n\u001b[1;32m   1133\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbest_of\u001b[39m\u001b[39m\"\u001b[39m: best_of,\n\u001b[1;32m   1134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mecho\u001b[39m\u001b[39m\"\u001b[39m: echo,\n\u001b[1;32m   1135\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfrequency_penalty\u001b[39m\u001b[39m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1136\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogit_bias\u001b[39m\u001b[39m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1137\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m: logprobs,\n\u001b[1;32m   1138\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1139\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: n,\n\u001b[1;32m   1140\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpresence_penalty\u001b[39m\u001b[39m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1141\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m: seed,\n\u001b[1;32m   1142\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop,\n\u001b[1;32m   1143\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream,\n\u001b[1;32m   1144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msuffix\u001b[39m\u001b[39m\"\u001b[39m: suffix,\n\u001b[1;32m   1145\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: temperature,\n\u001b[1;32m   1146\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mtop_p\u001b[39m\u001b[39m\"\u001b[39m: top_p,\n\u001b[1;32m   1147\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m: user,\n\u001b[1;32m   1148\u001b[0m             },\n\u001b[1;32m   1149\u001b[0m             completion_create_params\u001b[39m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1150\u001b[0m         ),\n\u001b[1;32m   1151\u001b[0m         options\u001b[39m=\u001b[39mmake_request_options(\n\u001b[1;32m   1152\u001b[0m             extra_headers\u001b[39m=\u001b[39mextra_headers, extra_query\u001b[39m=\u001b[39mextra_query, extra_body\u001b[39m=\u001b[39mextra_body, timeout\u001b[39m=\u001b[39mtimeout\n\u001b[1;32m   1153\u001b[0m         ),\n\u001b[1;32m   1154\u001b[0m         cast_to\u001b[39m=\u001b[39mCompletion,\n\u001b[1;32m   1155\u001b[0m         stream\u001b[39m=\u001b[39mstream \u001b[39mor\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m         stream_cls\u001b[39m=\u001b[39mAsyncStream[Completion],\n\u001b[1;32m   1157\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1542\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1529\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1530\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1537\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_AsyncStreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1538\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1539\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1540\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39m\u001b[39mawait\u001b[39;00m async_to_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[0;32m-> 1542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest(cast_to, opts, stream\u001b[39m=\u001b[39mstream, stream_cls\u001b[39m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1316\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1309\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     remaining_retries: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1315\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1317\u001b[0m         cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1318\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1319\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1320\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1321\u001b[0m         remaining_retries\u001b[39m=\u001b[39mremaining_retries,\n\u001b[1;32m   1322\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1354\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1353\u001b[0m     \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maclose()\n\u001b[0;32m-> 1354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m   1355\u001b[0m         options,\n\u001b[1;32m   1356\u001b[0m         cast_to,\n\u001b[1;32m   1357\u001b[0m         retries,\n\u001b[1;32m   1358\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1359\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1360\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1361\u001b[0m     )\n\u001b[1;32m   1363\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1424\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1420\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRetrying request to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m.\u001b[39murl, timeout)\n\u001b[1;32m   1422\u001b[0m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1424\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1425\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1426\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1427\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m   1428\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1429\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1430\u001b[0m )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1354\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1353\u001b[0m     \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maclose()\n\u001b[0;32m-> 1354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_request(\n\u001b[1;32m   1355\u001b[0m         options,\n\u001b[1;32m   1356\u001b[0m         cast_to,\n\u001b[1;32m   1357\u001b[0m         retries,\n\u001b[1;32m   1358\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m   1359\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1360\u001b[0m         stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1361\u001b[0m     )\n\u001b[1;32m   1363\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1424\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1420\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRetrying request to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m, options\u001b[39m.\u001b[39murl, timeout)\n\u001b[1;32m   1422\u001b[0m \u001b[39mawait\u001b[39;00m anyio\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1424\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\n\u001b[1;32m   1425\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   1426\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1427\u001b[0m     remaining_retries\u001b[39m=\u001b[39mremaining,\n\u001b[1;32m   1428\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m   1429\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1430\u001b[0m )\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.10/site-packages/openai/_base_client.py:1368\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m   1366\u001b[0m         \u001b[39mawait\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39maread()\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1370\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "while i < len(df_na.index):\n",
    "\n",
    "    print(f\" -- Starting Batch {i//batch_size + 1}\")\n",
    "    up = min(i + batch_size, len(df_na.index))\n",
    "    \n",
    "    batch_idx = df_na.index[i:up]\n",
    "    batch = list(inputs[batch_idx])\n",
    "    \n",
    "    start = time.time()\n",
    "    temp = await llm_chain.abatch(batch)\n",
    "    times.append(time.time() - start)\n",
    "\n",
    "    result = process_batch(temp)\n",
    "    for res in result:\n",
    "        df.iloc[res[0], -3:] = res[-1]\n",
    "        \n",
    "    df.to_csv(dst_path, index=False)\n",
    "    # print(res)\n",
    "        \n",
    "    i = i + batch_size\n",
    "    print(f\" -- Completed Batch {i//batch_size}: {times[-1]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>GLOBAL_ID</th>\n",
       "      <th>LOCAL_ID</th>\n",
       "      <th>processed_txt</th>\n",
       "      <th>date</th>\n",
       "      <th>_date_</th>\n",
       "      <th>WoY</th>\n",
       "      <th>travel_mode</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>3400</td>\n",
       "      <td>3400</td>\n",
       "      <td>6630</td>\n",
       "      <td>i am at mta subway cortlandt st wtc (1) in new...</td>\n",
       "      <td>2020-02-05 13:15:41+00:00</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>3401</td>\n",
       "      <td>3401</td>\n",
       "      <td>6631</td>\n",
       "      <td>not true. mta has reopened plenty of closed ex...</td>\n",
       "      <td>2020-02-05 13:15:31+00:00</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>3402</td>\n",
       "      <td>3402</td>\n",
       "      <td>6633</td>\n",
       "      <td>do your trains have a deadman switch? i am ask...</td>\n",
       "      <td>2020-02-05 13:12:04+00:00</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>3403</td>\n",
       "      <td>3403</td>\n",
       "      <td>6636</td>\n",
       "      <td>new pictures show the damage inside subway sta...</td>\n",
       "      <td>2020-02-05 13:09:58+00:00</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>3404</td>\n",
       "      <td>3404</td>\n",
       "      <td>6638</td>\n",
       "      <td>my subway conductor has told us to \"get in and...</td>\n",
       "      <td>2020-02-05 12:56:34+00:00</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>2020-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>3795</td>\n",
       "      <td>3795</td>\n",
       "      <td>7406</td>\n",
       "      <td>i am at mta nyct linden shop in brooklyn, ny h...</td>\n",
       "      <td>2020-05-30 08:48:45+00:00</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>3796</td>\n",
       "      <td>3796</td>\n",
       "      <td>7407</td>\n",
       "      <td>i used nyc subway system since 1977 so clean i...</td>\n",
       "      <td>2020-05-30 07:08:56+00:00</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>3797</td>\n",
       "      <td>3797</td>\n",
       "      <td>7409</td>\n",
       "      <td>no worries. the rioters are going to take over...</td>\n",
       "      <td>2020-05-30 06:06:00+00:00</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>3798</td>\n",
       "      <td>3798</td>\n",
       "      <td>7410</td>\n",
       "      <td>well, that is it for me, i am going to take th...</td>\n",
       "      <td>2020-05-30 05:56:45+00:00</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>3799</td>\n",
       "      <td>3799</td>\n",
       "      <td>7413</td>\n",
       "      <td>wow in what context you think this is fair? sh...</td>\n",
       "      <td>2020-05-30 04:36:37+00:00</td>\n",
       "      <td>2020-05-30</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  GLOBAL_ID  LOCAL_ID  \\\n",
       "400        3400       3400      6630   \n",
       "401        3401       3401      6631   \n",
       "402        3402       3402      6633   \n",
       "403        3403       3403      6636   \n",
       "404        3404       3404      6638   \n",
       "..          ...        ...       ...   \n",
       "795        3795       3795      7406   \n",
       "796        3796       3796      7407   \n",
       "797        3797       3797      7409   \n",
       "798        3798       3798      7410   \n",
       "799        3799       3799      7413   \n",
       "\n",
       "                                         processed_txt  \\\n",
       "400  i am at mta subway cortlandt st wtc (1) in new...   \n",
       "401  not true. mta has reopened plenty of closed ex...   \n",
       "402  do your trains have a deadman switch? i am ask...   \n",
       "403  new pictures show the damage inside subway sta...   \n",
       "404  my subway conductor has told us to \"get in and...   \n",
       "..                                                 ...   \n",
       "795  i am at mta nyct linden shop in brooklyn, ny h...   \n",
       "796  i used nyc subway system since 1977 so clean i...   \n",
       "797  no worries. the rioters are going to take over...   \n",
       "798  well, that is it for me, i am going to take th...   \n",
       "799  wow in what context you think this is fair? sh...   \n",
       "\n",
       "                          date      _date_      WoY travel_mode sentiment  \\\n",
       "400  2020-02-05 13:15:41+00:00  2020-02-05  2020-05         NaN       NaN   \n",
       "401  2020-02-05 13:15:31+00:00  2020-02-05  2020-05         NaN       NaN   \n",
       "402  2020-02-05 13:12:04+00:00  2020-02-05  2020-05         NaN       NaN   \n",
       "403  2020-02-05 13:09:58+00:00  2020-02-05  2020-05         NaN       NaN   \n",
       "404  2020-02-05 12:56:34+00:00  2020-02-05  2020-05         NaN       NaN   \n",
       "..                         ...         ...      ...         ...       ...   \n",
       "795  2020-05-30 08:48:45+00:00  2020-05-30  2020-21         NaN       NaN   \n",
       "796  2020-05-30 07:08:56+00:00  2020-05-30  2020-21         NaN       NaN   \n",
       "797  2020-05-30 06:06:00+00:00  2020-05-30  2020-21         NaN       NaN   \n",
       "798  2020-05-30 05:56:45+00:00  2020-05-30  2020-21         NaN       NaN   \n",
       "799  2020-05-30 04:36:37+00:00  2020-05-30  2020-21         NaN       NaN   \n",
       "\n",
       "    reasoning  \n",
       "400       NaN  \n",
       "401       NaN  \n",
       "402       NaN  \n",
       "403       NaN  \n",
       "404       NaN  \n",
       "..        ...  \n",
       "795       NaN  \n",
       "796       NaN  \n",
       "797       NaN  \n",
       "798       NaN  \n",
       "799       NaN  \n",
       "\n",
       "[400 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Results/openai/results_3000_3999.csv\")\n",
    "df[df['travel_mode'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    df.iloc[res[0], -3:] = res[-1]\n",
    "df.to_csv(dst_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
